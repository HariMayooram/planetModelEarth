import {
  LLM
} from "./chunk-OGEBVHA4.js";
import "./chunk-ICLFDNLI.js";
import "./chunk-JOAYDMNN.js";
import "./chunk-M3ACBAEH.js";
import "./chunk-LR3466M7.js";
import {
  GenerationChunk,
  IterableReadableStream,
  getEnvironmentVariable
} from "./chunk-ZU4UXHAA.js";
import "./chunk-EWTE5DHJ.js";

// node_modules/@langchain/community/dist/utils/event_source_parse.js
function isNodeJSReadable(x) {
  return x != null && typeof x === "object" && "on" in x;
}
async function getBytes(stream, onChunk) {
  if (isNodeJSReadable(stream)) {
    return new Promise((resolve) => {
      stream.on("readable", () => {
        let chunk;
        while (true) {
          chunk = stream.read();
          if (chunk == null) {
            onChunk(new Uint8Array(), true);
            break;
          }
          onChunk(chunk);
        }
        resolve();
      });
    });
  }
  const reader = stream.getReader();
  while (true) {
    const result = await reader.read();
    if (result.done) {
      onChunk(new Uint8Array(), true);
      break;
    }
    onChunk(result.value);
  }
}
function getLines(onLine) {
  let buffer;
  let position;
  let fieldLength;
  let discardTrailingNewline = false;
  return function onChunk(arr, flush) {
    if (flush) {
      onLine(arr, 0, true);
      return;
    }
    if (buffer === void 0) {
      buffer = arr;
      position = 0;
      fieldLength = -1;
    } else {
      buffer = concat(buffer, arr);
    }
    const bufLength = buffer.length;
    let lineStart = 0;
    while (position < bufLength) {
      if (discardTrailingNewline) {
        if (buffer[position] === 10) {
          lineStart = ++position;
        }
        discardTrailingNewline = false;
      }
      let lineEnd = -1;
      for (; position < bufLength && lineEnd === -1; ++position) {
        switch (buffer[position]) {
          case 58:
            if (fieldLength === -1) {
              fieldLength = position - lineStart;
            }
            break;
          case 13:
            discardTrailingNewline = true;
          case 10:
            lineEnd = position;
            break;
        }
      }
      if (lineEnd === -1) {
        break;
      }
      onLine(buffer.subarray(lineStart, lineEnd), fieldLength);
      lineStart = position;
      fieldLength = -1;
    }
    if (lineStart === bufLength) {
      buffer = void 0;
    } else if (lineStart !== 0) {
      buffer = buffer.subarray(lineStart);
      position -= lineStart;
    }
  };
}
function getMessages(onMessage, onId, onRetry) {
  let message = newMessage();
  const decoder = new TextDecoder();
  return function onLine(line, fieldLength, flush) {
    if (flush) {
      if (!isEmpty(message)) {
        onMessage == null ? void 0 : onMessage(message);
        message = newMessage();
      }
      return;
    }
    if (line.length === 0) {
      onMessage == null ? void 0 : onMessage(message);
      message = newMessage();
    } else if (fieldLength > 0) {
      const field = decoder.decode(line.subarray(0, fieldLength));
      const valueOffset = fieldLength + (line[fieldLength + 1] === 32 ? 2 : 1);
      const value = decoder.decode(line.subarray(valueOffset));
      switch (field) {
        case "data":
          message.data = message.data ? message.data + "\n" + value : value;
          break;
        case "event":
          message.event = value;
          break;
        case "id":
          onId == null ? void 0 : onId(message.id = value);
          break;
        case "retry": {
          const retry = parseInt(value, 10);
          if (!Number.isNaN(retry)) {
            onRetry == null ? void 0 : onRetry(message.retry = retry);
          }
          break;
        }
      }
    }
  };
}
function concat(a, b) {
  const res = new Uint8Array(a.length + b.length);
  res.set(a);
  res.set(b, a.length);
  return res;
}
function newMessage() {
  return {
    data: "",
    event: "",
    id: "",
    retry: void 0
  };
}
function convertEventStreamToIterableReadableDataStream(stream) {
  const dataStream = new ReadableStream({
    async start(controller) {
      const enqueueLine = getMessages((msg) => {
        if (msg.data)
          controller.enqueue(msg.data);
      });
      const onLine = (line, fieldLength, flush) => {
        enqueueLine(line, fieldLength, flush);
        if (flush)
          controller.close();
      };
      await getBytes(stream, getLines(onLine));
    }
  });
  return IterableReadableStream.fromReadableStream(dataStream);
}
function isEmpty(message) {
  return message.data === "" && message.event === "" && message.id === "" && message.retry === void 0;
}

// node_modules/@langchain/community/dist/llms/togetherai.js
var TogetherAI = class extends LLM {
  static lc_name() {
    return "TogetherAI";
  }
  constructor(inputs) {
    super(inputs);
    Object.defineProperty(this, "lc_serializable", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: true
    });
    Object.defineProperty(this, "temperature", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 0.7
    });
    Object.defineProperty(this, "topP", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 0.7
    });
    Object.defineProperty(this, "topK", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 50
    });
    Object.defineProperty(this, "modelName", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "model", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "streaming", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: false
    });
    Object.defineProperty(this, "repetitionPenalty", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 1
    });
    Object.defineProperty(this, "logprobs", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "maxTokens", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "safetyModel", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "stop", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "apiKey", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "inferenceAPIUrl", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: "https://api.together.xyz/inference"
    });
    const apiKey = inputs.apiKey ?? getEnvironmentVariable("TOGETHER_AI_API_KEY");
    if (!apiKey) {
      throw new Error("TOGETHER_AI_API_KEY not found.");
    }
    if (!inputs.model && !inputs.modelName) {
      throw new Error("Model name is required for TogetherAI.");
    }
    this.apiKey = apiKey;
    this.temperature = (inputs == null ? void 0 : inputs.temperature) ?? this.temperature;
    this.topK = (inputs == null ? void 0 : inputs.topK) ?? this.topK;
    this.topP = (inputs == null ? void 0 : inputs.topP) ?? this.topP;
    this.modelName = inputs.model ?? inputs.modelName ?? "";
    this.model = this.modelName;
    this.streaming = inputs.streaming ?? this.streaming;
    this.repetitionPenalty = inputs.repetitionPenalty ?? this.repetitionPenalty;
    this.logprobs = inputs.logprobs;
    this.safetyModel = inputs.safetyModel;
    this.maxTokens = inputs.maxTokens;
    this.stop = inputs.stop;
  }
  _llmType() {
    return "together_ai";
  }
  constructHeaders() {
    return {
      accept: "application/json",
      "content-type": "application/json",
      Authorization: `Bearer ${this.apiKey}`
    };
  }
  constructBody(prompt, options) {
    const body = {
      model: (options == null ? void 0 : options.model) ?? (options == null ? void 0 : options.modelName) ?? (this == null ? void 0 : this.model),
      prompt,
      temperature: (this == null ? void 0 : this.temperature) ?? (options == null ? void 0 : options.temperature),
      top_k: (this == null ? void 0 : this.topK) ?? (options == null ? void 0 : options.topK),
      top_p: (this == null ? void 0 : this.topP) ?? (options == null ? void 0 : options.topP),
      repetition_penalty: (this == null ? void 0 : this.repetitionPenalty) ?? (options == null ? void 0 : options.repetitionPenalty),
      logprobs: (this == null ? void 0 : this.logprobs) ?? (options == null ? void 0 : options.logprobs),
      stream_tokens: this == null ? void 0 : this.streaming,
      safety_model: (this == null ? void 0 : this.safetyModel) ?? (options == null ? void 0 : options.safetyModel),
      max_tokens: (this == null ? void 0 : this.maxTokens) ?? (options == null ? void 0 : options.maxTokens),
      stop: (this == null ? void 0 : this.stop) ?? (options == null ? void 0 : options.stop)
    };
    return body;
  }
  async completionWithRetry(prompt, options) {
    return this.caller.call(async () => {
      const fetchResponse = await fetch(this.inferenceAPIUrl, {
        method: "POST",
        headers: {
          ...this.constructHeaders()
        },
        body: JSON.stringify(this.constructBody(prompt, options))
      });
      if (fetchResponse.status === 200) {
        return fetchResponse.json();
      }
      const errorResponse = await fetchResponse.json();
      throw new Error(`Error getting prompt completion from Together AI. ${JSON.stringify(errorResponse, null, 2)}`);
    });
  }
  /** @ignore */
  async _call(prompt, options) {
    var _a, _b, _c;
    const response = await this.completionWithRetry(prompt, options);
    if (response.output) {
      return ((_a = response.output.choices[0]) == null ? void 0 : _a.text) ?? "";
    } else {
      return ((_c = (_b = response.choices) == null ? void 0 : _b[0]) == null ? void 0 : _c.text) ?? "";
    }
  }
  async *_streamResponseChunks(prompt, options, runManager) {
    const fetchResponse = await fetch(this.inferenceAPIUrl, {
      method: "POST",
      headers: {
        ...this.constructHeaders()
      },
      body: JSON.stringify(this.constructBody(prompt, options))
    });
    if (fetchResponse.status !== 200) {
      const errorResponse = await fetchResponse.json();
      throw new Error(`Error getting prompt completion from Together AI. ${JSON.stringify(errorResponse, null, 2)}`);
    }
    const stream = convertEventStreamToIterableReadableDataStream(fetchResponse.body);
    for await (const chunk of stream) {
      if (chunk !== "[DONE]") {
        const parsedChunk = JSON.parse(chunk);
        const generationChunk = new GenerationChunk({
          text: parsedChunk.choices[0].text ?? ""
        });
        yield generationChunk;
        void (runManager == null ? void 0 : runManager.handleLLMNewToken(generationChunk.text ?? ""));
      }
    }
  }
};
export {
  TogetherAI
};
//# sourceMappingURL=@langchain_community_llms_togetherai.js.map
