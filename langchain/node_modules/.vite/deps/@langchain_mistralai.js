import {
  require_http,
  require_mistralai
} from "./chunk-JLBOLGDJ.js";
import {
  JsonOutputKeyToolsParser,
  convertLangChainToolCallToOpenAI,
  makeInvalidToolCall,
  parseToolCall
} from "./chunk-SM6WFYUK.js";
import {
  JsonOutputParser,
  StructuredOutputParser
} from "./chunk-XQVUGUOI.js";
import "./chunk-BSQLF4IC.js";
import {
  LLM
} from "./chunk-OGEBVHA4.js";
import {
  Embeddings,
  chunkArray
} from "./chunk-VM2NCINP.js";
import {
  BaseChatModel
} from "./chunk-BBKWHEB7.js";
import {
  RunnablePassthrough
} from "./chunk-K33DWHXQ.js";
import "./chunk-ICLFDNLI.js";
import "./chunk-JOAYDMNN.js";
import "./chunk-M3ACBAEH.js";
import "./chunk-LR3466M7.js";
import {
  AIMessage,
  AIMessageChunk,
  AsyncCaller,
  ChatGenerationChunk,
  ChatMessageChunk,
  FunctionMessageChunk,
  GenerationChunk,
  HumanMessage,
  HumanMessageChunk,
  RunnableSequence,
  ToolMessageChunk,
  getEnvironmentVariable,
  isAIMessage,
  v4_default,
  zodToJsonSchema
} from "./chunk-ZU4UXHAA.js";
import {
  __toESM
} from "./chunk-EWTE5DHJ.js";

// node_modules/@langchain/mistralai/dist/chat_models.js
var import_mistralai = __toESM(require_mistralai(), 1);
var import_http = __toESM(require_http(), 1);

// node_modules/@langchain/mistralai/dist/utils.js
var TOOL_CALL_ID_PATTERN = /^[a-zA-Z0-9]{9}$/;
function _isValidMistralToolCallId(toolCallId) {
  return TOOL_CALL_ID_PATTERN.test(toolCallId);
}
function _base62Encode(num) {
  let numCopy = num;
  const base62 = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ";
  if (numCopy === 0)
    return base62[0];
  const arr = [];
  const base = base62.length;
  while (numCopy) {
    arr.push(base62[numCopy % base]);
    numCopy = Math.floor(numCopy / base);
  }
  return arr.reverse().join("");
}
function _simpleHash(str) {
  let hash = 0;
  for (let i = 0; i < str.length; i += 1) {
    const char = str.charCodeAt(i);
    hash = (hash << 5) - hash + char;
    hash &= hash;
  }
  return Math.abs(hash);
}
function _convertToolCallIdToMistralCompatible(toolCallId) {
  if (_isValidMistralToolCallId(toolCallId)) {
    return toolCallId;
  } else {
    const hash = _simpleHash(toolCallId);
    const base62Str = _base62Encode(hash);
    if (base62Str.length >= 9) {
      return base62Str.slice(0, 9);
    } else {
      return base62Str.padStart(9, "0");
    }
  }
}
function _mistralContentChunkToMessageContentComplex(content) {
  if (!content) {
    return "";
  }
  if (typeof content === "string") {
    return content;
  }
  return content.map((contentChunk) => {
    var _a;
    if (contentChunk.type === "image_url") {
      if (typeof contentChunk.imageUrl !== "string" && ((_a = contentChunk.imageUrl) == null ? void 0 : _a.detail)) {
        const { detail } = contentChunk.imageUrl;
        if (detail !== "high" && detail !== "auto" && detail !== "low") {
          return {
            type: contentChunk.type,
            image_url: {
              url: contentChunk.imageUrl.url
            }
          };
        }
      }
      return {
        type: contentChunk.type,
        image_url: contentChunk.imageUrl
      };
    }
    return contentChunk;
  });
}

// node_modules/@langchain/mistralai/dist/chat_models.js
function convertMessagesToMistralMessages(messages) {
  const getRole = (role) => {
    switch (role) {
      case "human":
        return "user";
      case "ai":
        return "assistant";
      case "system":
        return "system";
      case "tool":
        return "tool";
      case "function":
        return "assistant";
      default:
        throw new Error(`Unknown message type: ${role}`);
    }
  };
  const getContent = (content, type) => {
    const _generateContentChunk = (complex, role) => {
      if (complex.type === "image_url" && (role === "user" || role === "assistant")) {
        return {
          type: complex.type,
          imageUrl: complex == null ? void 0 : complex.image_url
        };
      }
      if (complex.type === "text") {
        return {
          type: complex.type,
          text: complex == null ? void 0 : complex.text
        };
      }
      throw new Error(`ChatMistralAI only supports messages of "image_url" for roles "user" and "assistant", and "text" for all others.

Received: ${JSON.stringify(content, null, 2)}`);
    };
    if (typeof content === "string") {
      return content;
    }
    if (Array.isArray(content)) {
      const mistralRole = getRole(type);
      const newContent = [];
      content.forEach((messageContentComplex) => {
        if (messageContentComplex.type === "text" || messageContentComplex.type === "image_url") {
          newContent.push(_generateContentChunk(messageContentComplex, mistralRole));
        } else {
          throw new Error(`Mistral only supports types "text" or "image_url" for complex message types.`);
        }
      });
      return newContent;
    }
    throw new Error(`Message content must be a string or an array.

Received: ${JSON.stringify(content, null, 2)}`);
  };
  const getTools = (message) => {
    var _a;
    if (isAIMessage(message) && !!((_a = message.tool_calls) == null ? void 0 : _a.length)) {
      return message.tool_calls.map((toolCall) => ({
        ...toolCall,
        id: _convertToolCallIdToMistralCompatible(toolCall.id ?? "")
      })).map(convertLangChainToolCallToOpenAI);
    }
    return void 0;
  };
  return messages.map((message) => {
    const toolCalls = getTools(message);
    const content = getContent(message.content, message.getType());
    if ("tool_call_id" in message && typeof message.tool_call_id === "string") {
      return {
        role: getRole(message.getType()),
        content,
        name: message.name,
        toolCallId: _convertToolCallIdToMistralCompatible(message.tool_call_id)
      };
    } else if (isAIMessage(message)) {
      if (toolCalls === void 0) {
        return {
          role: getRole(message.getType()),
          content
        };
      } else {
        return {
          role: getRole(message.getType()),
          toolCalls
        };
      }
    }
    return {
      role: getRole(message.getType()),
      content
    };
  });
}
function mistralAIResponseToChatMessage(choice, usage) {
  const { message } = choice;
  if (message === void 0) {
    throw new Error("No message found in response");
  }
  let rawToolCalls = [];
  if ("toolCalls" in message && Array.isArray(message.toolCalls)) {
    rawToolCalls = message.toolCalls;
  }
  const content = _mistralContentChunkToMessageContentComplex(message.content);
  switch (message.role) {
    case "assistant": {
      const toolCalls = [];
      const invalidToolCalls = [];
      for (const rawToolCall of rawToolCalls) {
        try {
          const parsed = parseToolCall(rawToolCall, { returnId: true });
          toolCalls.push({
            ...parsed,
            id: parsed.id ?? v4_default().replace(/-/g, "")
          });
        } catch (e) {
          invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));
        }
      }
      return new AIMessage({
        content,
        tool_calls: toolCalls,
        invalid_tool_calls: invalidToolCalls,
        additional_kwargs: {},
        usage_metadata: usage ? {
          input_tokens: usage.promptTokens,
          output_tokens: usage.completionTokens,
          total_tokens: usage.totalTokens
        } : void 0
      });
    }
    default:
      return new HumanMessage({ content });
  }
}
function _convertDeltaToMessageChunk(delta, usage) {
  var _a, _b, _c, _d;
  if (!delta.content && !delta.toolCalls) {
    if (usage) {
      return new AIMessageChunk({
        content: "",
        usage_metadata: usage ? {
          input_tokens: usage.promptTokens,
          output_tokens: usage.completionTokens,
          total_tokens: usage.totalTokens
        } : void 0
      });
    }
    return null;
  }
  const rawToolCallChunksWithIndex = ((_a = delta.toolCalls) == null ? void 0 : _a.length) ? (_b = delta.toolCalls) == null ? void 0 : _b.map((toolCall, index) => ({
    ...toolCall,
    index,
    id: toolCall.id ?? v4_default().replace(/-/g, ""),
    type: "function"
  })) : void 0;
  let role = "assistant";
  if (delta.role) {
    role = delta.role;
  }
  const content = _mistralContentChunkToMessageContentComplex(delta.content);
  let additional_kwargs;
  const toolCallChunks = [];
  if (rawToolCallChunksWithIndex !== void 0) {
    for (const rawToolCallChunk of rawToolCallChunksWithIndex) {
      const rawArgs = (_c = rawToolCallChunk.function) == null ? void 0 : _c.arguments;
      const args = rawArgs === void 0 || typeof rawArgs === "string" ? rawArgs : JSON.stringify(rawArgs);
      toolCallChunks.push({
        name: (_d = rawToolCallChunk.function) == null ? void 0 : _d.name,
        args,
        id: rawToolCallChunk.id,
        index: rawToolCallChunk.index,
        type: "tool_call_chunk"
      });
    }
  } else {
    additional_kwargs = {};
  }
  if (role === "user") {
    return new HumanMessageChunk({ content });
  } else if (role === "assistant") {
    return new AIMessageChunk({
      content,
      tool_call_chunks: toolCallChunks,
      additional_kwargs,
      usage_metadata: usage ? {
        input_tokens: usage.promptTokens,
        output_tokens: usage.completionTokens,
        total_tokens: usage.totalTokens
      } : void 0
    });
  } else if (role === "tool") {
    return new ToolMessageChunk({
      content,
      additional_kwargs,
      tool_call_id: (rawToolCallChunksWithIndex == null ? void 0 : rawToolCallChunksWithIndex[0].id) ?? ""
    });
  } else if (role === "function") {
    return new FunctionMessageChunk({
      content,
      additional_kwargs
    });
  } else {
    return new ChatMessageChunk({ content, role });
  }
}
function _convertToolToMistralTool(tools) {
  return tools.map((tool) => {
    if ("function" in tool) {
      return tool;
    }
    const description = tool.description ?? `Tool: ${tool.name}`;
    return {
      type: "function",
      function: {
        name: tool.name,
        description,
        parameters: zodToJsonSchema(tool.schema)
      }
    };
  });
}
var ChatMistralAI = class extends BaseChatModel {
  // Used for tracing, replace with the same name as your class
  static lc_name() {
    return "ChatMistralAI";
  }
  constructor(fields) {
    super(fields ?? {});
    Object.defineProperty(this, "lc_namespace", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: ["langchain", "chat_models", "mistralai"]
    });
    Object.defineProperty(this, "model", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: "mistral-small-latest"
    });
    Object.defineProperty(this, "apiKey", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "endpoint", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "serverURL", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "temperature", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 0.7
    });
    Object.defineProperty(this, "streaming", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: false
    });
    Object.defineProperty(this, "topP", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 1
    });
    Object.defineProperty(this, "maxTokens", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "safeMode", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: false
    });
    Object.defineProperty(this, "safePrompt", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: false
    });
    Object.defineProperty(this, "randomSeed", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "seed", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "maxRetries", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "lc_serializable", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: true
    });
    Object.defineProperty(this, "streamUsage", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: true
    });
    Object.defineProperty(this, "beforeRequestHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "requestErrorHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "responseHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "httpClient", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "presencePenalty", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "frequencyPenalty", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "numCompletions", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    const apiKey = (fields == null ? void 0 : fields.apiKey) ?? getEnvironmentVariable("MISTRAL_API_KEY");
    if (!apiKey) {
      throw new Error("API key MISTRAL_API_KEY is missing for MistralAI, but it is required.");
    }
    this.apiKey = apiKey;
    this.streaming = (fields == null ? void 0 : fields.streaming) ?? this.streaming;
    this.serverURL = (fields == null ? void 0 : fields.serverURL) ?? this.serverURL;
    this.temperature = (fields == null ? void 0 : fields.temperature) ?? this.temperature;
    this.topP = (fields == null ? void 0 : fields.topP) ?? this.topP;
    this.maxTokens = (fields == null ? void 0 : fields.maxTokens) ?? this.maxTokens;
    this.safePrompt = (fields == null ? void 0 : fields.safePrompt) ?? this.safePrompt;
    this.randomSeed = (fields == null ? void 0 : fields.seed) ?? (fields == null ? void 0 : fields.randomSeed) ?? this.seed;
    this.seed = this.randomSeed;
    this.maxRetries = fields == null ? void 0 : fields.maxRetries;
    this.httpClient = fields == null ? void 0 : fields.httpClient;
    this.model = (fields == null ? void 0 : fields.model) ?? (fields == null ? void 0 : fields.modelName) ?? this.model;
    this.streamUsage = (fields == null ? void 0 : fields.streamUsage) ?? this.streamUsage;
    this.beforeRequestHooks = (fields == null ? void 0 : fields.beforeRequestHooks) ?? this.beforeRequestHooks;
    this.requestErrorHooks = (fields == null ? void 0 : fields.requestErrorHooks) ?? this.requestErrorHooks;
    this.responseHooks = (fields == null ? void 0 : fields.responseHooks) ?? this.responseHooks;
    this.presencePenalty = (fields == null ? void 0 : fields.presencePenalty) ?? this.presencePenalty;
    this.frequencyPenalty = (fields == null ? void 0 : fields.frequencyPenalty) ?? this.frequencyPenalty;
    this.numCompletions = (fields == null ? void 0 : fields.numCompletions) ?? this.numCompletions;
    this.addAllHooksToHttpClient();
  }
  get lc_secrets() {
    return {
      apiKey: "MISTRAL_API_KEY"
    };
  }
  get lc_aliases() {
    return {
      apiKey: "mistral_api_key"
    };
  }
  getLsParams(options) {
    const params = this.invocationParams(options);
    return {
      ls_provider: "mistral",
      ls_model_name: this.model,
      ls_model_type: "chat",
      ls_temperature: params.temperature ?? void 0,
      ls_max_tokens: params.maxTokens ?? void 0
    };
  }
  _llmType() {
    return "mistral_ai";
  }
  /**
   * Get the parameters used to invoke the model
   */
  invocationParams(options) {
    const { response_format, tools, tool_choice } = options ?? {};
    const mistralAITools = (tools == null ? void 0 : tools.length) ? _convertToolToMistralTool(tools) : void 0;
    const params = {
      model: this.model,
      tools: mistralAITools,
      temperature: this.temperature,
      maxTokens: this.maxTokens,
      topP: this.topP,
      randomSeed: this.seed,
      safePrompt: this.safePrompt,
      toolChoice: tool_choice,
      responseFormat: response_format,
      presencePenalty: this.presencePenalty,
      frequencyPenalty: this.frequencyPenalty,
      n: this.numCompletions
    };
    return params;
  }
  bindTools(tools, kwargs) {
    return this.bind({
      tools: _convertToolToMistralTool(tools),
      ...kwargs
    });
  }
  async completionWithRetry(input, streaming) {
    const caller = new AsyncCaller({
      maxRetries: this.maxRetries
    });
    const client = new import_mistralai.Mistral({
      apiKey: this.apiKey,
      serverURL: this.serverURL,
      // If httpClient exists, pass it into constructor
      ...this.httpClient ? { httpClient: this.httpClient } : {}
    });
    return caller.call(async () => {
      var _a, _b, _c;
      try {
        let res;
        if (streaming) {
          res = await client.chat.stream(input);
        } else {
          res = await client.chat.complete(input);
        }
        return res;
      } catch (e) {
        if (((_a = e.message) == null ? void 0 : _a.includes("status: 400")) || ((_b = e.message) == null ? void 0 : _b.toLowerCase().includes("status 400")) || ((_c = e.message) == null ? void 0 : _c.includes("validation failed"))) {
          e.status = 400;
        }
        throw e;
      }
    });
  }
  /** @ignore */
  async _generate(messages, options, runManager) {
    var _a, _b;
    const tokenUsage = {};
    const params = this.invocationParams(options);
    const mistralMessages = convertMessagesToMistralMessages(messages);
    const input = {
      ...params,
      messages: mistralMessages
    };
    const shouldStream = options.signal ?? !!options.timeout;
    if (this.streaming || shouldStream) {
      const stream = this._streamResponseChunks(messages, options, runManager);
      const finalChunks = {};
      for await (const chunk of stream) {
        const index = ((_a = chunk.generationInfo) == null ? void 0 : _a.completion) ?? 0;
        if (finalChunks[index] === void 0) {
          finalChunks[index] = chunk;
        } else {
          finalChunks[index] = finalChunks[index].concat(chunk);
        }
      }
      const generations2 = Object.entries(finalChunks).sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10)).map(([_, value]) => value);
      return { generations: generations2, llmOutput: { estimatedTokenUsage: tokenUsage } };
    }
    const response = await this.completionWithRetry(input, false);
    const { completionTokens, promptTokens, totalTokens } = (response == null ? void 0 : response.usage) ?? {};
    if (completionTokens) {
      tokenUsage.completionTokens = (tokenUsage.completionTokens ?? 0) + completionTokens;
    }
    if (promptTokens) {
      tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;
    }
    if (totalTokens) {
      tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;
    }
    const generations = [];
    for (const part of (response == null ? void 0 : response.choices) ?? []) {
      if ("delta" in part) {
        throw new Error("Delta not supported in non-streaming mode.");
      }
      if (!("message" in part)) {
        throw new Error("No message found in the choice.");
      }
      let text = ((_b = part.message) == null ? void 0 : _b.content) ?? "";
      if (Array.isArray(text)) {
        text = text[0].type === "text" ? text[0].text : "";
      }
      const generation = {
        text,
        message: mistralAIResponseToChatMessage(part, response == null ? void 0 : response.usage)
      };
      if (part.finishReason) {
        generation.generationInfo = { finishReason: part.finishReason };
      }
      generations.push(generation);
    }
    return {
      generations,
      llmOutput: { tokenUsage }
    };
  }
  async *_streamResponseChunks(messages, options, runManager) {
    var _a;
    const mistralMessages = convertMessagesToMistralMessages(messages);
    const params = this.invocationParams(options);
    const input = {
      ...params,
      messages: mistralMessages
    };
    const streamIterable = await this.completionWithRetry(input, true);
    for await (const { data } of streamIterable) {
      if ((_a = options.signal) == null ? void 0 : _a.aborted) {
        throw new Error("AbortError");
      }
      const choice = data == null ? void 0 : data.choices[0];
      if (!choice || !("delta" in choice)) {
        continue;
      }
      const { delta } = choice;
      if (!delta) {
        continue;
      }
      const newTokenIndices = {
        prompt: 0,
        completion: choice.index ?? 0
      };
      const shouldStreamUsage = this.streamUsage || options.streamUsage;
      const message = _convertDeltaToMessageChunk(delta, shouldStreamUsage ? data.usage : null);
      if (message === null) {
        continue;
      }
      let text = delta.content ?? "";
      if (Array.isArray(text)) {
        text = text[0].type === "text" ? text[0].text : "";
      }
      const generationChunk = new ChatGenerationChunk({
        message,
        text,
        generationInfo: newTokenIndices
      });
      yield generationChunk;
      void (runManager == null ? void 0 : runManager.handleLLMNewToken(generationChunk.text ?? "", newTokenIndices, void 0, void 0, void 0, { chunk: generationChunk }));
    }
  }
  addAllHooksToHttpClient() {
    var _a, _b, _c;
    try {
      this.removeAllHooksFromHttpClient();
      const hasHooks = [
        this.beforeRequestHooks,
        this.requestErrorHooks,
        this.responseHooks
      ].some((hook) => hook && hook.length > 0);
      if (hasHooks && !this.httpClient) {
        this.httpClient = new import_http.HTTPClient();
      }
      if (this.beforeRequestHooks) {
        for (const hook of this.beforeRequestHooks) {
          (_a = this.httpClient) == null ? void 0 : _a.addHook("beforeRequest", hook);
        }
      }
      if (this.requestErrorHooks) {
        for (const hook of this.requestErrorHooks) {
          (_b = this.httpClient) == null ? void 0 : _b.addHook("requestError", hook);
        }
      }
      if (this.responseHooks) {
        for (const hook of this.responseHooks) {
          (_c = this.httpClient) == null ? void 0 : _c.addHook("response", hook);
        }
      }
    } catch {
      throw new Error("Error in adding all hooks");
    }
  }
  removeAllHooksFromHttpClient() {
    var _a, _b, _c;
    try {
      if (this.beforeRequestHooks) {
        for (const hook of this.beforeRequestHooks) {
          (_a = this.httpClient) == null ? void 0 : _a.removeHook("beforeRequest", hook);
        }
      }
      if (this.requestErrorHooks) {
        for (const hook of this.requestErrorHooks) {
          (_b = this.httpClient) == null ? void 0 : _b.removeHook("requestError", hook);
        }
      }
      if (this.responseHooks) {
        for (const hook of this.responseHooks) {
          (_c = this.httpClient) == null ? void 0 : _c.removeHook("response", hook);
        }
      }
    } catch {
      throw new Error("Error in removing hooks");
    }
  }
  removeHookFromHttpClient(hook) {
    var _a, _b, _c;
    try {
      (_a = this.httpClient) == null ? void 0 : _a.removeHook("beforeRequest", hook);
      (_b = this.httpClient) == null ? void 0 : _b.removeHook("requestError", hook);
      (_c = this.httpClient) == null ? void 0 : _c.removeHook("response", hook);
    } catch {
      throw new Error("Error in removing hook");
    }
  }
  /** @ignore */
  _combineLLMOutput() {
    return [];
  }
  withStructuredOutput(outputSchema, config) {
    const schema = outputSchema;
    const name = config == null ? void 0 : config.name;
    const method = config == null ? void 0 : config.method;
    const includeRaw = config == null ? void 0 : config.includeRaw;
    let llm;
    let outputParser;
    if (method === "jsonMode") {
      llm = this.bind({
        response_format: { type: "json_object" }
      });
      if (isZodSchema(schema)) {
        outputParser = StructuredOutputParser.fromZodSchema(schema);
      } else {
        outputParser = new JsonOutputParser();
      }
    } else {
      let functionName = name ?? "extract";
      if (isZodSchema(schema)) {
        const asJsonSchema = zodToJsonSchema(schema);
        llm = this.bind({
          tools: [
            {
              type: "function",
              function: {
                name: functionName,
                description: asJsonSchema.description,
                parameters: asJsonSchema
              }
            }
          ],
          tool_choice: "any"
        });
        outputParser = new JsonOutputKeyToolsParser({
          returnSingle: true,
          keyName: functionName,
          zodSchema: schema
        });
      } else {
        let openAIFunctionDefinition;
        if (typeof schema.name === "string" && typeof schema.parameters === "object" && schema.parameters != null) {
          openAIFunctionDefinition = schema;
          functionName = schema.name;
        } else {
          openAIFunctionDefinition = {
            name: functionName,
            description: schema.description ?? "",
            parameters: schema
          };
        }
        llm = this.bind({
          tools: [
            {
              type: "function",
              function: openAIFunctionDefinition
            }
          ],
          tool_choice: "any"
        });
        outputParser = new JsonOutputKeyToolsParser({
          returnSingle: true,
          keyName: functionName
        });
      }
    }
    if (!includeRaw) {
      return llm.pipe(outputParser);
    }
    const parserAssign = RunnablePassthrough.assign({
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      parsed: (input, config2) => outputParser.invoke(input.raw, config2)
    });
    const parserNone = RunnablePassthrough.assign({
      parsed: () => null
    });
    const parsedWithFallback = parserAssign.withFallbacks({
      fallbacks: [parserNone]
    });
    return RunnableSequence.from([
      {
        raw: llm
      },
      parsedWithFallback
    ]);
  }
};
function isZodSchema(input) {
  return typeof (input == null ? void 0 : input.parse) === "function";
}

// node_modules/@langchain/mistralai/dist/embeddings.js
var import_http2 = __toESM(require_http(), 1);
var MistralAIEmbeddings = class extends Embeddings {
  constructor(fields) {
    super(fields ?? {});
    Object.defineProperty(this, "modelName", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: "mistral-embed"
    });
    Object.defineProperty(this, "model", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: "mistral-embed"
    });
    Object.defineProperty(this, "encodingFormat", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: "float"
    });
    Object.defineProperty(this, "batchSize", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 512
    });
    Object.defineProperty(this, "stripNewLines", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: true
    });
    Object.defineProperty(this, "apiKey", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "endpoint", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "serverURL", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "beforeRequestHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "requestErrorHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "responseHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "httpClient", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    const apiKey = (fields == null ? void 0 : fields.apiKey) ?? getEnvironmentVariable("MISTRAL_API_KEY");
    if (!apiKey) {
      throw new Error("API key missing for MistralAI, but it is required.");
    }
    this.apiKey = apiKey;
    this.serverURL = (fields == null ? void 0 : fields.serverURL) ?? this.serverURL;
    this.modelName = (fields == null ? void 0 : fields.model) ?? (fields == null ? void 0 : fields.modelName) ?? this.model;
    this.model = this.modelName;
    this.encodingFormat = (fields == null ? void 0 : fields.encodingFormat) ?? this.encodingFormat;
    this.batchSize = (fields == null ? void 0 : fields.batchSize) ?? this.batchSize;
    this.stripNewLines = (fields == null ? void 0 : fields.stripNewLines) ?? this.stripNewLines;
    this.beforeRequestHooks = (fields == null ? void 0 : fields.beforeRequestHooks) ?? this.beforeRequestHooks;
    this.requestErrorHooks = (fields == null ? void 0 : fields.requestErrorHooks) ?? this.requestErrorHooks;
    this.responseHooks = (fields == null ? void 0 : fields.responseHooks) ?? this.responseHooks;
    this.httpClient = (fields == null ? void 0 : fields.httpClient) ?? this.httpClient;
    this.addAllHooksToHttpClient();
  }
  /**
   * Method to generate embeddings for an array of documents. Splits the
   * documents into batches and makes requests to the MistralAI API to generate
   * embeddings.
   * @param {Array<string>} texts Array of documents to generate embeddings for.
   * @returns {Promise<number[][]>} Promise that resolves to a 2D array of embeddings for each document.
   */
  async embedDocuments(texts) {
    const batches = chunkArray(this.stripNewLines ? texts.map((t) => t.replace(/\n/g, " ")) : texts, this.batchSize);
    const batchRequests = batches.map((batch) => this.embeddingWithRetry(batch));
    const batchResponses = await Promise.all(batchRequests);
    const embeddings = [];
    for (let i = 0; i < batchResponses.length; i += 1) {
      const batch = batches[i];
      const { data: batchResponse } = batchResponses[i];
      for (let j = 0; j < batch.length; j += 1) {
        embeddings.push(batchResponse[j].embedding ?? []);
      }
    }
    return embeddings;
  }
  /**
   * Method to generate an embedding for a single document. Calls the
   * embeddingWithRetry method with the document as the input.
   * @param {string} text Document to generate an embedding for.
   * @returns {Promise<number[]>} Promise that resolves to an embedding for the document.
   */
  async embedQuery(text) {
    const { data } = await this.embeddingWithRetry(this.stripNewLines ? text.replace(/\n/g, " ") : text);
    return data[0].embedding ?? [];
  }
  /**
   * Private method to make a request to the MistralAI API to generate
   * embeddings. Handles the retry logic and returns the response from the
   * API.
   * @param {string | Array<string>} inputs Text to send to the MistralAI API.
   * @returns {Promise<MistralAIEmbeddingsResponse>} Promise that resolves to the response from the API.
   */
  async embeddingWithRetry(inputs) {
    const { Mistral } = await this.imports();
    const client = new Mistral({
      apiKey: this.apiKey,
      serverURL: this.serverURL,
      // If httpClient exists, pass it into constructor
      ...this.httpClient ? { httpClient: this.httpClient } : {}
    });
    const embeddingsRequest = {
      model: this.model,
      inputs,
      encodingFormat: this.encodingFormat
    };
    return this.caller.call(async () => {
      const res = await client.embeddings.create(embeddingsRequest);
      return res;
    });
  }
  addAllHooksToHttpClient() {
    var _a, _b, _c;
    try {
      this.removeAllHooksFromHttpClient();
      const hasHooks = [
        this.beforeRequestHooks,
        this.requestErrorHooks,
        this.responseHooks
      ].some((hook) => hook && hook.length > 0);
      if (hasHooks && !this.httpClient) {
        this.httpClient = new import_http2.HTTPClient();
      }
      if (this.beforeRequestHooks) {
        for (const hook of this.beforeRequestHooks) {
          (_a = this.httpClient) == null ? void 0 : _a.addHook("beforeRequest", hook);
        }
      }
      if (this.requestErrorHooks) {
        for (const hook of this.requestErrorHooks) {
          (_b = this.httpClient) == null ? void 0 : _b.addHook("requestError", hook);
        }
      }
      if (this.responseHooks) {
        for (const hook of this.responseHooks) {
          (_c = this.httpClient) == null ? void 0 : _c.addHook("response", hook);
        }
      }
    } catch {
      throw new Error("Error in adding all hooks");
    }
  }
  removeAllHooksFromHttpClient() {
    var _a, _b, _c;
    try {
      if (this.beforeRequestHooks) {
        for (const hook of this.beforeRequestHooks) {
          (_a = this.httpClient) == null ? void 0 : _a.removeHook("beforeRequest", hook);
        }
      }
      if (this.requestErrorHooks) {
        for (const hook of this.requestErrorHooks) {
          (_b = this.httpClient) == null ? void 0 : _b.removeHook("requestError", hook);
        }
      }
      if (this.responseHooks) {
        for (const hook of this.responseHooks) {
          (_c = this.httpClient) == null ? void 0 : _c.removeHook("response", hook);
        }
      }
    } catch {
      throw new Error("Error in removing hooks");
    }
  }
  removeHookFromHttpClient(hook) {
    var _a, _b, _c;
    try {
      (_a = this.httpClient) == null ? void 0 : _a.removeHook("beforeRequest", hook);
      (_b = this.httpClient) == null ? void 0 : _b.removeHook("requestError", hook);
      (_c = this.httpClient) == null ? void 0 : _c.removeHook("response", hook);
    } catch {
      throw new Error("Error in removing hook");
    }
  }
  /** @ignore */
  async imports() {
    const { Mistral } = await import("./mistralai-IDG5USYM.js");
    return { Mistral };
  }
};

// node_modules/@langchain/mistralai/dist/llms.js
var import_http3 = __toESM(require_http(), 1);
var MistralAI = class extends LLM {
  static lc_name() {
    return "MistralAI";
  }
  constructor(fields) {
    super(fields ?? {});
    Object.defineProperty(this, "lc_namespace", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: ["langchain", "llms", "mistralai"]
    });
    Object.defineProperty(this, "lc_serializable", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: true
    });
    Object.defineProperty(this, "model", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: "codestral-latest"
    });
    Object.defineProperty(this, "temperature", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 0
    });
    Object.defineProperty(this, "topP", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "maxTokens", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "randomSeed", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "streaming", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: false
    });
    Object.defineProperty(this, "batchSize", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: 20
    });
    Object.defineProperty(this, "apiKey", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "endpoint", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "serverURL", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "maxRetries", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "maxConcurrency", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "beforeRequestHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "requestErrorHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "responseHooks", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    Object.defineProperty(this, "httpClient", {
      enumerable: true,
      configurable: true,
      writable: true,
      value: void 0
    });
    this.model = (fields == null ? void 0 : fields.model) ?? this.model;
    this.temperature = (fields == null ? void 0 : fields.temperature) ?? this.temperature;
    this.topP = (fields == null ? void 0 : fields.topP) ?? this.topP;
    this.maxTokens = (fields == null ? void 0 : fields.maxTokens) ?? this.maxTokens;
    this.randomSeed = (fields == null ? void 0 : fields.randomSeed) ?? this.randomSeed;
    this.batchSize = (fields == null ? void 0 : fields.batchSize) ?? this.batchSize;
    this.streaming = (fields == null ? void 0 : fields.streaming) ?? this.streaming;
    this.serverURL = (fields == null ? void 0 : fields.serverURL) ?? this.serverURL;
    this.maxRetries = fields == null ? void 0 : fields.maxRetries;
    this.maxConcurrency = fields == null ? void 0 : fields.maxConcurrency;
    this.beforeRequestHooks = (fields == null ? void 0 : fields.beforeRequestHooks) ?? this.beforeRequestHooks;
    this.requestErrorHooks = (fields == null ? void 0 : fields.requestErrorHooks) ?? this.requestErrorHooks;
    this.responseHooks = (fields == null ? void 0 : fields.responseHooks) ?? this.responseHooks;
    this.httpClient = (fields == null ? void 0 : fields.httpClient) ?? this.httpClient;
    const apiKey = (fields == null ? void 0 : fields.apiKey) ?? getEnvironmentVariable("MISTRAL_API_KEY");
    if (!apiKey) {
      throw new Error(`MistralAI requires an API key to be set.
Either provide one via the "apiKey" field in the constructor, or set the "MISTRAL_API_KEY" environment variable.`);
    }
    this.apiKey = apiKey;
    this.addAllHooksToHttpClient();
  }
  get lc_secrets() {
    return {
      apiKey: "MISTRAL_API_KEY"
    };
  }
  get lc_aliases() {
    return {
      apiKey: "mistral_api_key"
    };
  }
  _llmType() {
    return "mistralai";
  }
  invocationParams(options) {
    return {
      model: this.model,
      suffix: options.suffix,
      temperature: this.temperature,
      maxTokens: this.maxTokens,
      topP: this.topP,
      randomSeed: this.randomSeed,
      stop: options.stop
    };
  }
  /**
   * For some given input string and options, return a string output.
   *
   * Despite the fact that `invoke` is overridden below, we still need this
   * in order to handle public APi calls to `generate()`.
   */
  async _call(prompt, options) {
    var _a;
    const params = {
      ...this.invocationParams(options),
      prompt
    };
    const result = await this.completionWithRetry(params, options, false);
    let content = ((_a = result == null ? void 0 : result.choices) == null ? void 0 : _a[0].message.content) ?? "";
    if (Array.isArray(content)) {
      content = content[0].type === "text" ? content[0].text : "";
    }
    return content;
  }
  async _generate(prompts, options, runManager) {
    const subPrompts = chunkArray(prompts, this.batchSize);
    const choices = [];
    const params = this.invocationParams(options);
    for (let i = 0; i < subPrompts.length; i += 1) {
      const data = await (async () => {
        var _a;
        if (this.streaming) {
          const responseData = [];
          for (let x = 0; x < subPrompts[i].length; x += 1) {
            const choices2 = [];
            let response;
            const stream = await this.completionWithRetry({
              ...params,
              prompt: subPrompts[i][x]
            }, options, true);
            for await (const { data: data2 } of stream) {
              if (!response) {
                response = {
                  id: data2.id,
                  object: "chat.completion",
                  created: data2.created,
                  model: data2.model
                };
              }
              for (const part of data2.choices) {
                let content = part.delta.content ?? "";
                if (Array.isArray(content)) {
                  let strContent = "";
                  for (const contentChunk of content) {
                    if (contentChunk.type === "text") {
                      strContent += contentChunk.text;
                    } else if (contentChunk.type === "image_url") {
                      const imageURL = typeof contentChunk.imageUrl === "string" ? contentChunk.imageUrl : contentChunk.imageUrl.url;
                      strContent += imageURL;
                    }
                  }
                  content = strContent;
                }
                if (!choices2[part.index]) {
                  choices2[part.index] = {
                    index: part.index,
                    message: {
                      role: "assistant",
                      content,
                      toolCalls: null
                    },
                    finishReason: part.finishReason ?? "length"
                  };
                } else {
                  const choice = choices2[part.index];
                  choice.message.content += content;
                  choice.finishReason = part.finishReason ?? "length";
                }
                void (runManager == null ? void 0 : runManager.handleLLMNewToken(content, {
                  prompt: part.index,
                  completion: part.index
                }));
              }
            }
            if ((_a = options.signal) == null ? void 0 : _a.aborted) {
              throw new Error("AbortError");
            }
            responseData.push({
              ...response,
              choices: choices2
            });
          }
          return responseData;
        } else {
          const responseData = [];
          for (let x = 0; x < subPrompts[i].length; x += 1) {
            const res = await this.completionWithRetry({
              ...params,
              prompt: subPrompts[i][x]
            }, options, false);
            responseData.push(res);
          }
          return responseData;
        }
      })();
      choices.push(...data.map((d) => d.choices ?? []));
    }
    const generations = choices.map((promptChoices) => promptChoices.map((choice) => {
      var _a;
      let text = ((_a = choice.message) == null ? void 0 : _a.content) ?? "";
      if (Array.isArray(text)) {
        text = text[0].type === "text" ? text[0].text : "";
      }
      return {
        text,
        generationInfo: {
          finishReason: choice.finishReason
        }
      };
    }));
    return {
      generations
    };
  }
  async completionWithRetry(request, options, stream) {
    const { Mistral } = await this.imports();
    const caller = new AsyncCaller({
      maxConcurrency: options.maxConcurrency || this.maxConcurrency,
      maxRetries: this.maxRetries
    });
    const client = new Mistral({
      apiKey: this.apiKey,
      serverURL: this.serverURL,
      timeoutMs: options.timeout,
      // If httpClient exists, pass it into constructor
      ...this.httpClient ? { httpClient: this.httpClient } : {}
    });
    return caller.callWithOptions({
      signal: options.signal
    }, async () => {
      var _a, _b, _c;
      try {
        let res;
        if (stream) {
          res = await client.fim.stream(request);
        } else {
          res = await client.fim.complete(request);
        }
        return res;
      } catch (e) {
        if (((_a = e.message) == null ? void 0 : _a.includes("status: 400")) || ((_b = e.message) == null ? void 0 : _b.toLowerCase().includes("status 400")) || ((_c = e.message) == null ? void 0 : _c.includes("validation failed"))) {
          e.status = 400;
        }
        throw e;
      }
    });
  }
  async *_streamResponseChunks(prompt, options, runManager) {
    var _a;
    const params = {
      ...this.invocationParams(options),
      prompt
    };
    const stream = await this.completionWithRetry(params, options, true);
    for await (const message of stream) {
      const { data } = message;
      const choice = data == null ? void 0 : data.choices[0];
      if (!choice) {
        continue;
      }
      let text = choice.delta.content ?? "";
      if (Array.isArray(text)) {
        text = text[0].type === "text" ? text[0].text : "";
      }
      const chunk = new GenerationChunk({
        text,
        generationInfo: {
          finishReason: choice.finishReason,
          tokenUsage: data.usage
        }
      });
      yield chunk;
      void (runManager == null ? void 0 : runManager.handleLLMNewToken(chunk.text ?? ""));
    }
    if ((_a = options.signal) == null ? void 0 : _a.aborted) {
      throw new Error("AbortError");
    }
  }
  addAllHooksToHttpClient() {
    var _a, _b, _c;
    try {
      this.removeAllHooksFromHttpClient();
      const hasHooks = [
        this.beforeRequestHooks,
        this.requestErrorHooks,
        this.responseHooks
      ].some((hook) => hook && hook.length > 0);
      if (hasHooks && !this.httpClient) {
        this.httpClient = new import_http3.HTTPClient();
      }
      if (this.beforeRequestHooks) {
        for (const hook of this.beforeRequestHooks) {
          (_a = this.httpClient) == null ? void 0 : _a.addHook("beforeRequest", hook);
        }
      }
      if (this.requestErrorHooks) {
        for (const hook of this.requestErrorHooks) {
          (_b = this.httpClient) == null ? void 0 : _b.addHook("requestError", hook);
        }
      }
      if (this.responseHooks) {
        for (const hook of this.responseHooks) {
          (_c = this.httpClient) == null ? void 0 : _c.addHook("response", hook);
        }
      }
    } catch {
      throw new Error("Error in adding all hooks");
    }
  }
  removeAllHooksFromHttpClient() {
    var _a, _b, _c;
    try {
      if (this.beforeRequestHooks) {
        for (const hook of this.beforeRequestHooks) {
          (_a = this.httpClient) == null ? void 0 : _a.removeHook("beforeRequest", hook);
        }
      }
      if (this.requestErrorHooks) {
        for (const hook of this.requestErrorHooks) {
          (_b = this.httpClient) == null ? void 0 : _b.removeHook("requestError", hook);
        }
      }
      if (this.responseHooks) {
        for (const hook of this.responseHooks) {
          (_c = this.httpClient) == null ? void 0 : _c.removeHook("response", hook);
        }
      }
    } catch {
      throw new Error("Error in removing hooks");
    }
  }
  removeHookFromHttpClient(hook) {
    var _a, _b, _c;
    try {
      (_a = this.httpClient) == null ? void 0 : _a.removeHook("beforeRequest", hook);
      (_b = this.httpClient) == null ? void 0 : _b.removeHook("requestError", hook);
      (_c = this.httpClient) == null ? void 0 : _c.removeHook("response", hook);
    } catch {
      throw new Error("Error in removing hook");
    }
  }
  /** @ignore */
  async imports() {
    const { Mistral } = await import("./mistralai-IDG5USYM.js");
    return { Mistral };
  }
};
export {
  ChatMistralAI,
  MistralAI,
  MistralAIEmbeddings
};
//# sourceMappingURL=@langchain_mistralai.js.map
